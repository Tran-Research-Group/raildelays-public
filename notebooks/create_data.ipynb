{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:23:30.436836Z",
     "start_time": "2020-03-24T17:23:30.429647Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jacobheglund/dev/raildelays-public\n"
     ]
    }
   ],
   "source": [
    "# external libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir = \"/home/jacobheglund/dev/raildelays-public\"\n",
    "\n",
    "# change directory for loading files from disk and loading custom dictionaries\n",
    "sys.path.append(base_dir)\n",
    "os.chdir(base_dir)\n",
    "print(os.getcwd())\n",
    "\n",
    "# custom libraries\n",
    "from src.utils.utils import z_score\n",
    "from src.data.data_processing import data_interface\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:23:32.381152Z",
     "start_time": "2020-03-24T17:23:32.377734Z"
    }
   },
   "outputs": [],
   "source": [
    "# set global options\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "figsize = (5, 4)\n",
    "dpi = 300\n",
    "node_size = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:23:32.532788Z",
     "start_time": "2020-03-24T17:23:32.526171Z"
    }
   },
   "outputs": [],
   "source": [
    "# include all these data points\n",
    "raw_dir = \"./data/raw/\"\n",
    "interim_dir = \"./data/interim/\"\n",
    "processed_dir = \"./data/processed/\"\n",
    "Path(raw_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(interim_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(processed_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# input data locations\n",
    "raw_data_path_2016 = raw_dir + \"HSP_2016_DID_PAD.txt\"\n",
    "raw_data_path_2017 = raw_dir + \"HSP_2017_DID_PAD.txt\"\n",
    "stop_location_path = raw_dir + \"stations.csv\"\n",
    "\n",
    "# save locations\n",
    "adjlist_path = interim_dir + \"adjlist.txt\"\n",
    "adj_path = processed_dir + \"adj.npy\"\n",
    "dataset_path = processed_dir + \"dataset.npy\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link-Based Node Formulation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocssing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:23:42.117861Z",
     "start_time": "2020-03-24T17:23:33.684566Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(raw_data_path_2016, sep=\",\")\n",
    "df2 = pd.read_csv(raw_data_path_2017, sep=\",\")\n",
    "df2 = df2.drop([\"Unnamed: 0\"], axis=1)\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "# 2017 includes XXXPAD and PADXXX trips, 2016 does not, so remove the outbound trips\n",
    "df = df[df[\"station_origin\"] != \"PAD\"]\n",
    "\n",
    "# remove any stops that are included in either year but not the other\n",
    "stops_2016 = np.unique(df1[\"station_curr\"])\n",
    "stops_2017 = np.unique(df2[\"station_curr\"])\n",
    "stops_drop = []\n",
    "\n",
    "for i in stops_2017:\n",
    "    if i not in stops_2016:\n",
    "        stops_drop.append(i)\n",
    "for i in stops_2016:\n",
    "    if i not in stops_2017:\n",
    "        stops_drop.append(i)\n",
    "for i in stops_drop:\n",
    "    df = df[df[\"station_curr\"] != i]\n",
    "    \n",
    "\n",
    "# remove stations that serve an average of less than stop_thres trains per day \n",
    "stations = np.unique(df[\"station_curr\"])\n",
    "n_days = len(np.unique(df[\"date\"]))\n",
    "# stops per station\n",
    "sps = dict(collections.Counter(df[\"station_curr\"]))\n",
    "sps = dict(sorted(sps.items()))\n",
    "\n",
    "# if you change this value you will have to manually change the corridors and available routes since\n",
    "# that process is difficult to automate at this point in time\n",
    "stop_thres = 1\n",
    "counter = 0\n",
    "for i in list(sps):\n",
    "    if (sps[i] / n_days) < stop_thres:\n",
    "        # find set of RID's associated with these\n",
    "        df_tmp = df.loc[df[\"station_curr\"] == i]\n",
    "        RID_drop = np.unique(df_tmp[\"RID\"])\n",
    "        # remove train routes that stop at any of the non-included stations\n",
    "        for j in RID_drop:\n",
    "            df = df.loc[df[\"RID\"] != j]\n",
    "        sps.pop(i)\n",
    "\n",
    "df = df.reset_index()\n",
    "df = df.drop([\"index\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:23:42.189991Z",
     "start_time": "2020-03-24T17:23:42.119054Z"
    }
   },
   "outputs": [],
   "source": [
    "stops_data_period = np.unique(df[\"station_curr\"])\n",
    "df_sp = pd.read_csv(stop_location_path)\n",
    "G_stop2location = {}\n",
    "for i in range(len(df_sp)):\n",
    "    curr_row = df_sp[i:i+1]\n",
    "    if curr_row[\"crs\"].item() in stops_data_period:\n",
    "        G_stop2location[curr_row[\"crs\"].item()] = (curr_row[\"lat\"].item(), curr_row[\"lon\"].item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:23:42.200399Z",
     "start_time": "2020-03-24T17:23:42.191524Z"
    }
   },
   "outputs": [],
   "source": [
    "# all corridors are defined in linear order\n",
    "## once a train gets on a corridor, it travels through all the stations on that corridor\n",
    "## although it does not necessarily stop at any of the stations on that corridor\n",
    "\n",
    "# the corridors are designed with the threshold of at least 1 train per day on average\n",
    "## it keeps out the \"end of line\" stations which just add noise to the data\n",
    "## in total this removes ~1% of the data\n",
    "\n",
    "# list of railway stations\n",
    "## https://en.wikipedia.org/wiki/UK_railway_stations_%E2%80%93_A\n",
    "\n",
    "# station map for defining corridors\n",
    "## https://www.gwr.com/plan-journey/stations-and-routes \n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# Northwest Corridor\n",
    "# --------------------------------------------\n",
    "# SWA - SWANSEA - START OF CORRIDOR\n",
    "# NTH - NEATH\n",
    "# PTA - PORT TALBOT PARKWAY\n",
    "# BGN - BRIDGEND\n",
    "# CDF - CARDIFF CENTRAL\n",
    "# NWP - NEWPORT\n",
    "# BPW - BRISTOL PARKWAY\n",
    "# SWI - SWINDON - END OF CORRIDOR\n",
    "\n",
    "# --------------------------------------------\n",
    "# Southwest Corridor\n",
    "# --------------------------------------------\n",
    "# WSM - WESTON-SUPER-MARE - START OF CORRIDOR\n",
    "# WNM - WESTON MILTON\n",
    "# WOR - WORLE\n",
    "# YAT - YATTON\n",
    "# NLS - NAILSEA AND BLACKWELL\n",
    "# BRI - BRISTON TEMPLE MEADS\n",
    "# BTH - BATH SPA\n",
    "# CPM - CHIPPENHAM\n",
    "# SWI - SWINDON - END OF CORRIDOR\n",
    "\n",
    "# --------------------------------------------\n",
    "# Middle Connecting Corridor\n",
    "# --------------------------------------------\n",
    "# SWI - SWINDON - START OF CORRIDOR\n",
    "# DID - DIDCOT PARKWAY - END OF CORRIDOR\n",
    "\n",
    "# --------------------------------------------\n",
    "# North Central Corridor\n",
    "# --------------------------------------------\n",
    "# BAN - BANBURY - START OF CORRIDOR\n",
    "# KGS - KINGS SUTTON\n",
    "# HYD - HEYFORD\n",
    "# TAC - TACKLEY\n",
    "# OXF - OXFORD\n",
    "# RAD - RADLEY\n",
    "# CUM - CULHAM\n",
    "# APF - APPLEFORD\n",
    "# DID - DIDCOT PARKWAY - END OF CORRIDOR\n",
    "\n",
    "# --------------------------------------------\n",
    "# Eastern Corridor\n",
    "# --------------------------------------------\n",
    "# DID - DIDCOT PARKWAY - START OF CORRIDOR\n",
    "# CHO - CHOLSEY\n",
    "# GOR - GORING AND STREATLEY\n",
    "# PAN - PANGBOURNE\n",
    "# TLH - TILEHURST\n",
    "# RDG - READING\n",
    "# TWY - TWYFORD\n",
    "# MAI - MAIDENHEAD\n",
    "# BNM - BURNHAM\n",
    "# SLO - SLOUGH\n",
    "# LNY - LANGLEY\n",
    "# IVR - IVER\n",
    "# WDT - WEST DRAYTON\n",
    "# HAY - HAYES AND HARLINGTON\n",
    "# STL - SOUTHALL\n",
    "# EAL - EALING BROADWAY\n",
    "# PAD - LONDON PADDINGTON - END OF CORRIDOR\n",
    "\n",
    "# used in defining the graph\n",
    "corridor_list = [\n",
    "    [\"SWA\", \"NTH\", \"PTA\", \"BGN\", \"CDF\", \"NWP\", \"BPW\", \"SWI\"],\n",
    "    [\"WSM\", \"WNM\", \"WOR\", \"YAT\", \"NLS\", \"BRI\", \"BTH\", \"CPM\", \"SWI\"],\n",
    "    [\"SWI\", \"DID\"],\n",
    "    [\"BAN\", \"KGS\", \"HYD\", \"TAC\", \"OXF\", \"RAD\", \"CUM\", \"APF\", \"DID\"],\n",
    "    [\"DID\", \"CHO\", \"GOR\", \"PAN\", \"TLH\", \"RDG\", \"TWY\", \"MAI\", \"BNM\", \"SLO\", \"LNY\",\n",
    "     \"IVR\", \"WDT\", \"HAY\", \"STL\", \"EAL\", \"PAD\"]\n",
    "]\n",
    "\n",
    "# rearrange sps with stops grouped by corridor\n",
    "sps_tmp = {}\n",
    "for corridor in corridor_list:\n",
    "    for j in corridor:\n",
    "        keys = list(sps_tmp)\n",
    "        if j not in keys:\n",
    "            sps_tmp[j] = sps[j]\n",
    "sps = sps_tmp\n",
    "\n",
    "# used in finding links between stops\n",
    "available_routes = [\n",
    "    [\"SWA\", \"NTH\", \"PTA\", \"BGN\", \"CDF\", \"NWP\", \"BPW\", \"SWI\", \"DID\", \"CHO\",\n",
    "     \"GOR\", \"PAN\", \"TLH\", \"RDG\", \"TWY\", \"MAI\", \"BNM\", \"SLO\", \"LNY\",\n",
    "     \"IVR\", \"WDT\", \"HAY\", \"STL\", \"EAL\", \"PAD\"],\n",
    "    \n",
    "    [\"WSM\", \"WNM\", \"WOR\", \"YAT\", \"NLS\", \"BRI\", \"BTH\", \n",
    "     \"CPM\", \"SWI\", \"DID\", \"CHO\", \"GOR\", \"PAN\", \"TLH\", \"RDG\", \"TWY\", \"MAI\",\n",
    "     \"BNM\", \"SLO\", \"LNY\", \"IVR\", \"WDT\", \"HAY\", \"STL\", \"EAL\", \"PAD\"],\n",
    "    \n",
    "    [\"BAN\", \"KGS\", \"HYD\", \"TAC\", \"OXF\", \"RAD\", \"CUM\", \"APF\", \"DID\", \"CHO\",\n",
    "     \"GOR\", \"PAN\", \"TLH\", \"RDG\", \"TWY\", \"MAI\", \"BNM\", \"SLO\", \"LNY\",\n",
    "     \"IVR\", \"WDT\", \"HAY\", \"STL\", \"EAL\", \"PAD\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:23:42.207135Z",
     "start_time": "2020-03-24T17:23:42.202380Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_links(stop_1, stop_2, available_routes):\n",
    "    # returns a list of all links between two stops on a route\n",
    "    # this relies on having an array of possible routes\n",
    "    #EX: my_list_1 = get_links(\"WDT\", \"PAD\")\n",
    "    # my_list_1 = [\"WDTHHAY\", \"HAYSTL\", \"STLEAL\", \"EALPAD\"]\n",
    "    done = 0\n",
    "    link_list = []\n",
    "    for i in range(len(available_routes)):\n",
    "        curr_route = available_routes[i]\n",
    "        if (stop_1 in curr_route) and (stop_2 in curr_route) and (not done):\n",
    "            done = 1\n",
    "            stop_1_idx = curr_route.index(stop_1)\n",
    "            stop_2_idx = curr_route.index(stop_2)            \n",
    "            for j in range(stop_1_idx, stop_2_idx):                \n",
    "                link_list.append(curr_route[j] + curr_route[j+1])\n",
    "        \n",
    "    return link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:23:42.482542Z",
     "start_time": "2020-03-24T17:23:42.208601Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dataframe setup\n",
    "\n",
    "# we're using scheduled times, but there is no reason that we couldn't use actual arrival/departure times\n",
    "## the reason we don't is b/c we would have scheduled tiems available in the real world prediction\n",
    "## maybe a better way would be to use scheduled times in the future predictions,\n",
    "## and actual times in for input data but this just adds needless complecation for a first stab at the problem\n",
    "\n",
    "# OD does not index the links, just the origin and destination of the route\n",
    "df[\"OD\"] = df[\"station_origin\"] + df[\"station_destination\"]\n",
    "\n",
    "# get integer time values for all rows (arrival and departure)\n",
    "df[\"dep_sched_int\"] = df[\"departure_sched\"]\n",
    "df[\"dep_sched_int\"][df[\"dep_sched_int\"] == \"terminating\"] = df[\"arrival_sched\"]\n",
    "df[\"dep_actual_int\"] = df[\"departure_actual\"]\n",
    "df[\"dep_actual_int\"][df[\"dep_actual_int\"] == \"terminating\"] = df[\"arrival_actual\"]\n",
    "\n",
    "# this one is different b/c \"starting\" arrival delay must be 0\n",
    "df[\"arr_sched_int\"] = df[\"arrival_sched\"]\n",
    "df[\"arr_sched_int\"][df[\"arr_sched_int\"] == \"starting\"] = df[\"departure_actual\"]\n",
    "df[\"arr_actual_int\"] = df[\"arrival_actual\"]\n",
    "df[\"arr_actual_int\"][df[\"arr_actual_int\"] == \"starting\"] = df[\"departure_actual\"]\n",
    "\n",
    "# get scheduled arrival and departure as datetime\n",
    "df[\"arr_sched_datetime\"] = pd.to_datetime(df[\"date\"].astype(str) + \" \" + df[\"arr_sched_int\"].astype(str))\n",
    "df[\"dep_sched_datetime\"] = pd.to_datetime(df[\"date\"].astype(str) + \" \" + df[\"dep_sched_int\"].astype(str))\n",
    "df[\"arr_actual_datetime\"] = pd.to_datetime(df[\"date\"].astype(str) + \" \" + df[\"arr_actual_int\"].astype(str))\n",
    "df[\"dep_actual_datetime\"] = pd.to_datetime(df[\"date\"].astype(str) + \" \" + df[\"dep_actual_int\"].astype(str)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:24:43.910735Z",
     "start_time": "2020-03-24T17:23:42.483942Z"
    }
   },
   "outputs": [],
   "source": [
    "# find the delay that was generated by the the previous section of track for each stop\n",
    "## arrival delay is positive if the train was late, and negative if the train was early\n",
    "\n",
    "# also get the departure time of the previous station (we need this to attribute the delay to the proper\n",
    "# time periods)\n",
    "\n",
    "# get the set of links each train travelled over between consecutive stops\n",
    "df[\"cum_arr_delay\"] = ((df[\"arr_actual_datetime\"] - df[\"arr_sched_datetime\"]).dt.total_seconds()) / 60\n",
    "init_list = []\n",
    "for i in range(len(df)):\n",
    "    init_list.append([])\n",
    "df[\"prev_links\"] = init_list\n",
    "\n",
    "\n",
    "# make sure we're only combining data from one train trip at a time\n",
    "for row_count in range(len(df)):\n",
    "    curr_row = df[row_count:row_count+1]\n",
    "    if (curr_row[\"arrival_sched\"].item() == \"starting\"):\n",
    "        df.at[row_count, \"prev_link_avg_datetime\"] = df.at[row_count, \"dep_sched_datetime\"]\n",
    "        df.at[row_count, \"prev_link_delay\"] = 0.\n",
    "        df.at[row_count, \"prev_links\"] = []\n",
    "        df.at[row_count, \"prev_station_dep_sched_datetime\"] = df.at[row_count, \"dep_sched_datetime\"]\n",
    "\n",
    "    else:\n",
    "        prev_row = df[row_count-1:row_count]\n",
    "\n",
    "        t0 = prev_row[\"dep_sched_datetime\"].item()\n",
    "        t1 = curr_row[\"arr_sched_datetime\"].item()\n",
    "\n",
    "        # average time at which the train was running on previous link\n",
    "        df.at[row_count, \"prev_link_avg_datetime\"] = t0 + ((t1-t0) / 2)\n",
    "        prev_links = get_links(prev_row[\"station_curr\"].item(), curr_row[\"station_curr\"].item(), available_routes)\n",
    "        df.at[row_count, \"prev_links\"] = prev_links\n",
    "        \n",
    "        # departure time from previous station\n",
    "        df.at[row_count, \"prev_station_dep_sched_datetime\"] = prev_row[\"dep_sched_datetime\"].item()\n",
    "\n",
    "        # average delay per link\n",
    "        # average the delay over # of links passed through by trains\n",
    "        ## Train 2: A -> D (doesn't stop at B or C but passes through)\n",
    "        ## 6 minute arrival delay at D\n",
    "        ## average delay of 6 minutes / 3 links = 2 minutes/link is attributed to each link b/c the data isn't\n",
    "        ## granular enough to provide a more accurate estimation\n",
    "        n_links = len(prev_links)\n",
    "        if (n_links == 0):\n",
    "            n_links = 1\n",
    "            \n",
    "        df.at[row_count, \"prev_link_delay\"] = (curr_row[\"cum_arr_delay\"].item() - prev_row[\"cum_arr_delay\"].item()) \\\n",
    "        / n_links\n",
    "        \n",
    "    row_count += 1\n",
    "        \n",
    "# alphabetize columns\n",
    "df = df.reindex(sorted(df.columns), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of Rail Network Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:24:43.914580Z",
     "start_time": "2020-03-24T17:24:43.911901Z"
    }
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "G_stop2idx = {}\n",
    "G_idx2stop = {}\n",
    "for i in list(sps):\n",
    "    G_stop2idx[i] = counter\n",
    "    G_idx2stop[counter] = str(i)\n",
    "    counter += 1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:24:43.920746Z",
     "start_time": "2020-03-24T17:24:43.916139Z"
    }
   },
   "outputs": [],
   "source": [
    "# # processing to verify the corridors I defined\n",
    "# # Southwest Corridor\n",
    "# # check if any trains starting in TAU go through BPW or if they all go through SWI\n",
    "# ## Result: They all go through SWI, so the SW Corridor goes from TAU to SWI without branching off in this dataset\n",
    "# ## Result: The Northwest and Southwest Corridors only meet at SWI for this dataset\n",
    "\n",
    "# # North Central Corridor\n",
    "# ## all trains going from BAN to PAD must pass through DID to be part of this dataset\n",
    "# ## therefore there are no trains going from APF to CHO\n",
    "# ## no trains go from BAN and pass through SWI\n",
    "\n",
    "# df1 = df.copy()\n",
    "# df1 = df1.loc[df1[\"station_origin\"] == \"BAN\"]\n",
    "# df1 = df1.loc[df1[\"station_curr\"] == \"SWI\"]\n",
    "\n",
    "\n",
    "# df1.head()\n",
    "# print(len(df1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:24:43.927981Z",
     "start_time": "2020-03-24T17:24:43.921764Z"
    }
   },
   "outputs": [],
   "source": [
    "# create undirected graph representing inbound rail network\n",
    "\n",
    "total_str = \"\"\n",
    "for corridor in corridor_list:\n",
    "    for j in range(len(corridor)):\n",
    "        tmp_str = \"\"\n",
    "        if (j == 0):\n",
    "            tmp_str += str(G_stop2idx[corridor[j]])\n",
    "            tmp_str += \" \"\n",
    "            tmp_str += str(G_stop2idx[corridor[j+1]])\n",
    "\n",
    "        elif (j == len(corridor)-1):\n",
    "            tmp_str += str(G_stop2idx[corridor[j]])\n",
    "            tmp_str += \" \"\n",
    "            tmp_str += str(G_stop2idx[corridor[j-1]])\n",
    "\n",
    "        else:\n",
    "            tmp_str += str(G_stop2idx[corridor[j]])\n",
    "            tmp_str += \" \"\n",
    "            tmp_str += str(G_stop2idx[corridor[j-1]])\n",
    "            tmp_str += \" \"\n",
    "            tmp_str += str(G_stop2idx[corridor[j+1]])\n",
    "\n",
    "        total_str += tmp_str\n",
    "        total_str += \"\\n\"\n",
    "\n",
    "with open(adjlist_path, \"w\") as f:\n",
    "    f.write(total_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:24:43.934460Z",
     "start_time": "2020-03-24T17:24:43.929253Z"
    }
   },
   "outputs": [],
   "source": [
    "G_idx2location = {}\n",
    "counter = 0\n",
    "for i in G_stop2location:\n",
    "    G_idx2location[G_stop2idx[i]] = (G_stop2location[i][1], G_stop2location[i][0])\n",
    "\n",
    "G = nx.read_adjlist(adjlist_path, nodetype=int)\n",
    "LG = nx.line_graph(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:24:43.942244Z",
     "start_time": "2020-03-24T17:24:43.936086Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the position of the edges as average of station positions\n",
    "LG_node2location = {}\n",
    "for edge in LG.nodes():\n",
    "    station_1 = int(edge[0])\n",
    "    station_2 = int(edge[1])\n",
    "    x1, x2 = G_idx2location[station_1][0], G_idx2location[station_2][0]\n",
    "    y1, y2 = G_idx2location[station_1][1], G_idx2location[station_2][1]\n",
    "    \n",
    "    x_edge = (x1+x2)/2\n",
    "    y_edge = (y1+y2)/2\n",
    "    LG_node2location[edge] = (x_edge, y_edge) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:24:43.947432Z",
     "start_time": "2020-03-24T17:24:43.943240Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get edge labels\n",
    "LG_node2label = {}\n",
    "for edge in LG.nodes():\n",
    "    station_1 = int(edge[0])\n",
    "    station_2 = int(edge[1])\n",
    "    \n",
    "    # i'm not sure why these two cases mess it up, but\n",
    "    # SWICPM should be CPMSWI\n",
    "    # DIDAPF should be APFDID\n",
    "    # something about the fact that they have multiple connections or are part of a complex junction\n",
    "\n",
    "    if (G_idx2stop[station_1] == \"SWI\") and (G_idx2stop[station_2] == \"CPM\") or \\\n",
    "        (G_idx2stop[station_1] == \"DID\") and (G_idx2stop[station_2] == \"APF\"):\n",
    "        station_tmp = station_1\n",
    "        station_1 = station_2\n",
    "        station_2 = station_tmp\n",
    "    \n",
    "    LG_node2label[edge] = G_idx2stop[station_1] + G_idx2stop[station_2]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:24:43.957568Z",
     "start_time": "2020-03-24T17:24:43.948519Z"
    }
   },
   "outputs": [],
   "source": [
    "# add self loops to graph\n",
    "for i in LG.nodes:\n",
    "    LG.add_edge(i, i)\n",
    "\n",
    "adj = nx.adjacency_matrix(LG).todense()\n",
    "adj = np.array(adj).astype(\"d\")\n",
    "np.save(adj_path, adj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of Rail Link Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:24:44.118061Z",
     "start_time": "2020-03-24T17:24:43.958835Z"
    }
   },
   "outputs": [],
   "source": [
    "## find starting and ending time for the day\n",
    "def round_time(t_input, dt, direction=\"floor\"):\n",
    "    # \n",
    "    if direction == \"floor\":\n",
    "        n_minutes = ((pd.to_timedelta(str(t_input[0:2] + \":\" + t_input[2:4] + \":00\")).seconds / 60) // dt * dt)\n",
    "        return pd.to_timedelta(n_minutes, unit=\"m\")\n",
    "    \n",
    "    elif direction == \"ceil\":\n",
    "        n_minutes = ((pd.to_timedelta(str(t_input[0:2] + \":\" + t_input[2:4] + \":00\")).seconds / 60 + dt) // dt * dt)\n",
    "        return pd.to_timedelta(n_minutes, unit=\"m\")\n",
    "\n",
    "dt = 10 # minutes\n",
    "    \n",
    "t_min = min(df[\"dep_sched_int\"])\n",
    "t_max = max(df[\"dep_sched_int\"])\n",
    "t_start = round_time(t_min, dt)\n",
    "t_end = round_time(t_max, dt, \"ceil\")\n",
    "\n",
    "# unique times during each day\n",
    "time_list = []\n",
    "t_curr = t_start\n",
    "\n",
    "while t_curr < t_end:\n",
    "    time_list.append(t_curr)\n",
    "    t_curr += pd.Timedelta(str(dt) + \"minutes\")\n",
    "\n",
    "# unique dates of the year\n",
    "date_list = np.unique(df[\"date\"])\n",
    "\n",
    "datetime_list = []\n",
    "for i in date_list:\n",
    "    t0 = pd.to_datetime(i + \" 00:00:00\")\n",
    "        \n",
    "    for j in time_list:\n",
    "        t_curr = t0 + j\n",
    "        datetime_list.append(t_curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:24:44.131558Z",
     "start_time": "2020-03-24T17:24:44.119190Z"
    }
   },
   "outputs": [],
   "source": [
    "## construct useful mappings\n",
    "# mapping from node description to unique index\n",
    "LG_node2idx = {}\n",
    "# mapping from unique index to node description \n",
    "LG_idx2node = {}\n",
    "counter = 0\n",
    "for i in LG.nodes():\n",
    "    LG_node2idx[i] = counter\n",
    "    LG_idx2node[counter] = i\n",
    "    counter += 1\n",
    "\n",
    "\n",
    "# mapping from node description to label (i.e. \"SWAPAD\")\n",
    "LG_node_label = LG_node2label\n",
    "# mapping from label to node description\n",
    "LG_label2node = {}\n",
    "for i in LG_node_label:\n",
    "    LG_label2node[LG_node_label[i]] = i\n",
    "\n",
    "\n",
    "# mapping from time of year to time data index\n",
    "datetime2idx = {}\n",
    "# mapping from time data index to time of year\n",
    "idx2datetime = {}\n",
    "counter = 0\n",
    "for i in datetime_list:\n",
    "    datetime2idx[i] = counter\n",
    "    idx2datetime[counter] = i\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:26:17.942324Z",
     "start_time": "2020-03-24T17:24:44.132691Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_nodes = len(adj)\n",
    "n_timesteps = len(datetime_list)\n",
    "n_features = 1 #  delay per link\n",
    "n_timesteps_per_day = ((t_end - t_start)/dt).seconds / 60\n",
    "delay_dataset = np.zeros((n_timesteps, n_nodes, n_features))\n",
    "n_trains_total = np.zeros((n_nodes, 1))\n",
    "n_trains_link_dt = np.zeros((n_timesteps, n_nodes, 1))\n",
    "\n",
    "for i in range(len(datetime_list)-1):\n",
    "    # get set of trains running during current time period    \n",
    "    t0, t1 = datetime_list[i], datetime_list[i+1]\n",
    "    \n",
    "    # attribute the link delay to all relevant times \n",
    "    ## i.e. from scheduled departure of previous station to scheduled arrival at current station\n",
    "    t_avg_cond = ((t0 <= df[\"prev_link_avg_datetime\"]) & (df[\"prev_link_avg_datetime\"] <= t1))\n",
    "    t_arr_cond = ((t0 <= df[\"arr_sched_datetime\"]) & (df[\"arr_sched_datetime\"] <= t1))\n",
    "    t_dep_cond = ((t0 <= df[\"prev_station_dep_sched_datetime\"]) & (df[\"prev_station_dep_sched_datetime\"] <= t1))\n",
    "    \n",
    "    df_tmp = df.loc[t_avg_cond | t_dep_cond | t_arr_cond]\n",
    "    n_trains_link = np.zeros((n_nodes, 1))\n",
    "\n",
    "    for j in range(len(df_tmp)):\n",
    "        curr_row = df_tmp[j:j+1]\n",
    "        prev_links = curr_row[\"prev_links\"].item()\n",
    "\n",
    "        for k in range(len(prev_links)):\n",
    "            link = prev_links[k]\n",
    "            link_idx = LG_node2idx[LG_label2node[link]]\n",
    "\n",
    "            # sum the average delay experienced by trains passing through link during the time period\n",
    "            delay_dataset[i, link_idx, 0] += curr_row[\"prev_link_delay\"].item()\n",
    "            \n",
    "            # enumerate trains passing through each link during the time period\n",
    "            n_trains_link[link_idx] += 1\n",
    "    n_trains_link_dt[i, :, :] = n_trains_link\n",
    "    n_trains_total += n_trains_link\n",
    "    \n",
    "    # calculate delay over each link averaged by number of trains that passed through the link during the time period\n",
    "    divisor = n_trains_link + np.where(n_trains_link == 0, 1, 0)\n",
    "    delay_dataset[i, :, :] = delay_dataset[i, :, :] / divisor\n",
    "\n",
    "np.save(dataset_path, delay_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:26:17.946594Z",
     "start_time": "2020-03-24T17:26:17.943569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_timesteps_per_day: 42.0\n",
      "n_nodes 40\n"
     ]
    }
   ],
   "source": [
    "print(\"n_timesteps_per_day:\", n_timesteps_per_day)\n",
    "print(\"n_nodes\", n_nodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Visualization for ITSC Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Adjacency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:11:29.006515Z",
     "start_time": "2020-03-11T22:11:28.720632Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "\n",
    "plt.imshow(adj)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Number of traversals per link during 2016 and 2017\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:11:29.292780Z",
     "start_time": "2020-03-11T22:11:29.007936Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "label2traversals = {}\n",
    "idx2traversals = {}\n",
    "\n",
    "for i in range(len(n_trains_total)):\n",
    "    label2traversals[LG_node_label[LG_idx2node[i]]] = int(n_trains_total[i].item())\n",
    "    idx2traversals[i] = int(n_trains_total[i].item())\n",
    "    \n",
    "idx2traversals = {k: v for k, v in sorted(idx2traversals.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "label2traversals = {k: v for k, v in sorted(label2traversals.items(), key=lambda item: item[1], reverse=True)}\n",
    "labels, y = zip(*label2traversals.items())\n",
    "labels = list(labels)\n",
    "for i in range(len(labels)):\n",
    "    labels[i] = labels[i][0:3] + \"-\" + labels[i][3:]\n",
    "    \n",
    "x = np.arange(0, len(y))\n",
    "x = sorted(x, reverse=True)\n",
    "y = sorted(y, reverse=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3.4, 4), dpi=dpi)\n",
    "plt.barh(x,y)\n",
    "# plt.ylabel(\"Link Name\")\n",
    "plt.xlabel(\"Attributed Link Traversals\", fontsize=6)\n",
    "plt.xticks(fontsize=5)\n",
    "plt.yticks(fontsize=5)\n",
    "ax.set(yticks=x, yticklabels=labels)\n",
    "plt.grid(axis=\"x\")\n",
    "# plt.title(\"Link Traversals for 2016 and 2017\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Model Accuracy Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:11:29.298205Z",
     "start_time": "2020-03-11T22:11:29.294037Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fp = \"./paper_data/experiment_results.csv\"\n",
    "df_acc = pd.read_csv(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:11:29.328203Z",
     "start_time": "2020-03-11T22:11:29.299676Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_names = np.unique(df_acc[\"Model\"])\n",
    "timesteps_in = np.unique(df_acc[\"Timesteps In\"])\n",
    "\n",
    "for i in model_names:\n",
    "    print(\"\\n\\n\", i)\n",
    "    df_tmp1 = df_acc.loc[df_acc[\"Model\"] == i]\n",
    "    for j in timesteps_in:\n",
    "        print(\"Timesteps In\", j)\n",
    "        df_tmp2 = df_tmp1.loc[df_tmp1[\"Timesteps In\"] == j]\n",
    "\n",
    "        print(\"MAE\")\n",
    "        mae_str = \"\"\n",
    "        for k in range(len(df_tmp2)):\n",
    "            curr_row = df_tmp2[k: k+1]\n",
    "            mae_str += str(round(curr_row[\"MAE (test)\"].item(), 3))\n",
    "            mae_str += \" / \"\n",
    "        print(mae_str)\n",
    "        \n",
    "        print(\"RMSE\")\n",
    "        rmse_str = \"\"\n",
    "        for k in range(len(df_tmp2)):\n",
    "            curr_row = df_tmp2[k: k+1]\n",
    "            rmse_str += str(round(curr_row[\"RMSE (test)\"].item(), 3))\n",
    "            rmse_str += \" / \"\n",
    "        print(rmse_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Plot of Graph and Line Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:11:29.998979Z",
     "start_time": "2020-03-11T22:11:29.329964Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib import collections  as mc\n",
    "# fig, ax = plt.subplots(nrows=1 ,ncols=3,figsize=(7.5, 2.5), dpi=dpi)\n",
    "\n",
    "xlim = (-3.99, 0)\n",
    "ylim = (51.32, 52.08)\n",
    "\n",
    "fig = plt.figure(tight_layout=True, figsize=(7.05, 3.75), dpi=dpi)\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "\n",
    "ax = fig.add_subplot(gs[0, 0])\n",
    "nx.draw(G, G_idx2location, node_size=15, width=0.8, color=\"b\", edge_color=\"r\")\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "plt.text(-4,52, \"(a)\")\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(gs[0, 1])\n",
    "nx.draw(LG, LG_node2location, node_size=15, width=0.8, node_color=\"r\", edge_color=\"b\")\n",
    "# draw rectangle for the zoomed image\n",
    "rect = Rectangle((-0.8,51.44),0.7,0.15,linewidth=2,edgecolor='k',facecolor='None')\n",
    "ax.add_patch(rect)\n",
    "plt.text(-4,52, \"(b)\")\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "plt.tight_layout()\n",
    "\n",
    "ax = fig.add_subplot(gs[1, :])\n",
    "plt.sca(ax)\n",
    "nx.draw(LG, LG_node2location, node_size=0, width=0.8, node_color=\"r\", edge_color=\"b\")\n",
    "nx.draw_networkx_labels(LG, LG_node2location, LG_node2label, font_size=8, font_weight=\"bold\")\n",
    "nx.draw_networkx_edges(LG, LG_node2location, edge_color=\"b\")\n",
    "\n",
    "# these work for zooming in around London\n",
    "xlim = (-0.71, -0.21)\n",
    "ylim = (51.503, 51.522)\n",
    "rect = Rectangle((-0.7091,51.5033),0.49,0.0186,linewidth=2,edgecolor='k',facecolor='None')\n",
    "ax.add_patch(rect)\n",
    "plt.text(-0.707,51.5033 + 0.0192, \"(c)\")\n",
    "\n",
    "# drawing line works, but it squishes the image vertically which isn't ideal for a paper\n",
    "# # draw lines pointing from the small rectangle to the large rectangle\n",
    "# # vertical line\n",
    "# # bottom point\n",
    "# x1, y1 = (-0.707+0.47, 51.503 + 0.0187), (-0.707+0.47, 51.503+0.0187)\n",
    "# # top point\n",
    "# x2, y2 = (-0.707+0.49, 51.503 + 0.037), (-0.707+0.49, 51.503+0.037)\n",
    "\n",
    "# # slanted line\n",
    "# #bottom point\n",
    "# x3, y3 = (-0.707+0.4, 51.503 + 0.0187), (-0.707+0.4, 51.503+0.0187)\n",
    "# # top point\n",
    "# x4, y4 = (-0.707+0.45, 51.503+0.037), (-0.707+0.45, 51.503+0.037)\n",
    "\n",
    "\n",
    "# lines = [[x1, y1, x2, y2], [x3, y3, x4, y4]]\n",
    "# # lines = [[x1, y1, x2, y2]]\n",
    "# lc = mc.LineCollection(lines, linewidths = 1, color=\"k\", zorder=0)\n",
    "# lc.set_clip_on(False)\n",
    "# ax.add_collection(lc)\n",
    "\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Other Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:11:30.003721Z",
     "start_time": "2020-03-11T22:11:30.001371Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# xlim, ylim based on min/max lat/long for the included stations in the dataset\n",
    "xlim = (-4.05, -0.05)\n",
    "ylim = (51.3, 52.08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Visalize Graph and Line Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:11:30.206034Z",
     "start_time": "2020-03-11T22:11:30.004905Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## graph with dots at station locations\n",
    "fig = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "nx.draw(G, G_idx2location, node_size=node_size, width=0.4)\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:11:30.492916Z",
     "start_time": "2020-03-11T22:11:30.207247Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## graph with labels at station locations\n",
    "fig = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "nx.draw_networkx_labels(G, G_idx2location, G_idx2stop, font_size=3)\n",
    "nx.draw_networkx_edges(G, G_idx2location, width=0.1)\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:11:30.683670Z",
     "start_time": "2020-03-11T22:11:30.493957Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## line graph with dots at link locations\n",
    "fig = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "nx.draw(LG, LG_node2location, node_size=node_size, width=0.4)\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:11:30.972289Z",
     "start_time": "2020-03-11T22:11:30.684878Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## line grpah with labels at link locations\n",
    "fig, ax = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "nx.draw_networkx_labels(LG, LG_node2location, LG_node2label, font_size=3)\n",
    "nx.draw_networkx_edges(LG, LG_node2location, width=0.1)\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Actual delay vs predicted delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:11:32.313145Z",
     "start_time": "2020-03-11T22:11:30.973509Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# first find a really busy time of the day\n",
    "total_trains_per_link = np.sum(n_trains_link_dt, axis=1)\n",
    "# 68 links were traversed on the entire network during this 10 minute period\n",
    "\n",
    "busy_idx = np.argmax(total_trains_per_link)\n",
    "curr_date = datetime_list[busy_idx]\n",
    "date_str = str(curr_date.year) + \"-0\" + str(curr_date.month) + \"-\" + str(curr_date.day)\n",
    "\n",
    "df_plot = df.loc[df[\"date\"] == date_str]\n",
    "\n",
    "# get time at which we will collect data\n",
    "datetime_plot = []\n",
    "t0 = pd.to_datetime(date_str + \" 00:00:00\")\n",
    "for j in time_list:\n",
    "    t_curr = t0 + j\n",
    "    datetime_plot.append(t_curr)\n",
    "    \n",
    "# get date indicies of delay_dataset for visualization of model \n",
    "idx_plot = []\n",
    "for i in datetime_plot:\n",
    "    if i in datetime_list:\n",
    "        idx_plot.append(datetime_list.index(i))\n",
    "\n",
    "datetime_plot_str = []\n",
    "for i in datetime_plot:\n",
    "    datetime_plot_str.append(str(i)[11:])\n",
    "data_plot = delay_dataset[idx_plot, :, :]\n",
    "fig, ax = plt.subplots(figsize=(16, 9), dpi=dpi)\n",
    "\n",
    "# plot the delay state for the n_plot busiest links during an average day\n",
    "x_ticks = np.arange(0, data_plot.shape[0])\n",
    "n_plot = 1\n",
    "counter = 0\n",
    "for i in list(idx2traversals):\n",
    "    if counter < n_plot:\n",
    "        plt.bar(x_ticks,data_plot[:, i].squeeze(), alpha=0.5)\n",
    "        plt.xticks(x_ticks)\n",
    "        ax.set_xticklabels(datetime_plot_str, rotation=90)\n",
    "        counter += 1\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Training Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:11:32.317255Z",
     "start_time": "2020-03-11T22:11:32.314490Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "in_list = [6, 12]\n",
    "out_list= [1, 3, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:11:32.454019Z",
     "start_time": "2020-03-11T22:11:32.318978Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in in_list:\n",
    "    for j in out_list:\n",
    "        fp = \"./paper_data/training_curves/in_{}_out_{}_run-.-tag-Mean Absolute Error (testing).csv\".format(i, j)\n",
    "        df_train = pd.read_csv(fp)\n",
    "        plt.plot(df_train[\"Step\"], df_train[\"Value\"], label=(i, j))\n",
    "        plt.xlabel(\"Training Epoch\")\n",
    "        plt.ylabel(\"Test MAE\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:11:32.590444Z",
     "start_time": "2020-03-11T22:11:32.455351Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in in_list:\n",
    "    for j in out_list:\n",
    "        fp = \"./paper_data/training_curves/in_{}_out_{}_run-.-tag-Root Mean Squared Error (testing).csv\".format(i, j)\n",
    "        df_train = pd.read_csv(fp)\n",
    "        plt.plot(df_train[\"Step\"], df_train[\"Value\"], label=(i, j))\n",
    "        plt.xlabel(\"Training Epoch\")\n",
    "        plt.ylabel(\"Test RMSE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:11:32.727533Z",
     "start_time": "2020-03-11T22:11:32.591514Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in in_list:\n",
    "    for j in out_list:\n",
    "        fp = \"./paper_data/training_curves/in_{}_out_{}_run-.-tag-Average Training Loss.csv\".format(i, j)\n",
    "        df_train = pd.read_csv(fp)\n",
    "        plt.plot(df_train[\"Step\"], df_train[\"Value\"], label=(i, j))\n",
    "        plt.xlabel(\"Training Epoch\")\n",
    "        plt.ylabel(\"Training Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
